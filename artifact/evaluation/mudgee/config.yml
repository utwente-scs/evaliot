general:
    method-name: "MUDgee"
    input-type: "per_device"
    log-dir: "./logs"

data-ingestion:
    list-datasets:
      - name: "TrainData"
        path: "<path-to-the-train-dataset"
        type: "per_device"
        load: no # yes = Load the list of pcap files, no = the method will take directory as input
      - name: "TestData"
        path: "<path-to-the-test-dataset"
        type: "per_device"
        load: no # yes = Load the list of pcap files, no = the method will take directory as input

data-preprocessing:
    required-data-format: raw # [json, csv] (options)
    use-known-devices: no # IF you want to only test on devices available in the training data
    train-dataset: "TrainData"  # can be different from train_data
    test-dataset: "TestData"
    threshold:
      - 0.8 # Dynamic similarity score
      - 0.7 # Static similarity score

model-training:
    class-name: "MudgeeTraining"
    class-path: "framework/eval_modules/mudgee_training.py"  # Relative path to the class file
    train-model: no # Run the training or use already trained models
    # If the method needs a shell script to run the training. Empty if not.
    cmds:
      # Format it with device directory and result file name
      merge: "sh ./merge-pcaps.sh {} {}"
      # Format it with dataset-name, iteration-number and file-dict ({device:"file-path"})
      update: "python update-config.py . {} '{}' {}" 
      # Format it with dataset-name
      train: "sh ./run-mudgee.sh . {}" # IoT-Sentinel 
    paths:
      repo: "<path-to-the-code-repository>"
      eval-dir: "./evaluation/mudgee"
      model-dir: "./evaluation/mudgee/{}_result" # Path to where the models are stored.

model-testing:
    class-name: "MudgeeTesting"
    class-path: "framework/eval_modules/mudgee_testing.py"
    report-dir: "./evaluation/mudgee/reports/"
    default-gateway-ip: "192.168.12.1"
